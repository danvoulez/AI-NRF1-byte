version: 1
name: llm-engine
includes: ["llm-engine-bundle"]
backend: { mode: in-process }
pipeline:
  - use: cap-intake
    with: { map: { query: "@inputs.prompt" } }
  - use: policy.light
  - use: cap-structure
  - use: cap-llm-engine
    with: { model: "gpt-4o-mini", max_tokens: 4096, latency_budget_ms: 1500 }
  - use: cap-enrich
  - use: cap-transport
outputs: { format: "json", fields: ["artifacts","metrics","receipt_cid","url_rica"] }
