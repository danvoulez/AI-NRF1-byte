# 0) Sum√°rio executivo (1 min)

* **Modelo can√¥nico**: 3 ATOS (**ATTEST / EVALUATE / TRANSACT**).
* **TDLN = c√©rebro**: toda a√ß√£o/decis√£o passa por TDpasted01:08Looking at this blueprint against everything we've built ‚Äî the engine, the wrapper template v2, SIRP, the 3-act model, the product manifesto ‚Äî here's what I see:

What's solid and should ship as-is
The Execution Permit is the right move. Right now the engine evaluates and emits a receipt, but nothing binds the decision to the actual execution. The permit closes that gap ‚Äî the executor won't move without a signed, time-bounded, hash-pinned permit from TDLN. That's the difference between "we decided to allow it" and "we provably authorized this specific action with this specific input." Every product needs this.
The state machine is clean. NEW ‚Üí (REQUIRE? consent : ALLOW) ‚Üí READY ‚Üí RUNNING ‚Üí DONE|ERROR ‚Äî this covers every product. Insurance underwriting that needs k-of-n approvals? REQUIRE with consent. Automated API receipt gateway? Straight ALLOW ‚Üí READY. Credit scoring that hits a threshold edge case? ASK maps to REQUIRE. One state machine, every product.
The ledger layout is right. NDJSON by date with ULID keys, separate paths for requests/consent/ready/receipts/logs ‚Äî that's the canonical audit trail. Every product writes to the same structure, different enrichments read from it.
The adapter pattern (CP‚ÜíTDLN) preserves the existing TDLN dialect while letting the Control Plane speak product language. No rewrite needed for P0. That's correct.

What I'd change
The act schemas are too flat. Your attest.schema.json has subject, properties, evidence, context at the top level. But when you compose acts ‚Äî insurance is ATTEST‚ÜíEVALUATE, credit is ATTEST‚ÜíEVALUATE, auditing is EVALUATE over past receipts ‚Äî the schemas need to nest. I'd add a pipeline field:
json{
  "act": "EVALUATE",
  "pipeline_prev": "act_01HZ_attest_result_cid",
  "subject": { ... },
  "rules_ref": [ ... ]
}
This way the EVALUATE act can reference the ATTEST receipt that preceded it. The hash chain links them. Auditing becomes an EVALUATE where `subject` is a list of receipt CIDs from past acts. The engine doesn't change ‚Äî `pipeline_prev` is just another field in the input JSON that policies can reference.

**The P0 scope is too infrastructure-focused.** Control API + Ledger + Executor + Dashboard is the ops layer. It's necessary but it doesn't produce a product. I'd split P0 into two tracks:

- **P0-infra**: Control API, Ledger, Executor with permit, Dashboard (your current P0)
- **P0-product**: One real product end-to-end using the wrapper template v2 + act templates + manifesto

Ship them in parallel. P0-infra gives you the operational backbone. P0-product proves the manifesto works ‚Äî that you can go from `product.json` to a running service that accepts input, evaluates policies, emits receipts with SIRP capsules, and renders a ghost or badge.

**The consent model needs the k-of-n semantics specified.** You mention "1/2 grants ‚Üí READY" but the schema doesn't define how k and n are set per action type. The engine already has `KOfN` aggregation. Wire it: the product manifesto should include `consent: { k: 2, n: 3, roles: ["compliance_officer", "manager", "legal"] }` when the act requires REQUIRE. Different products have different consent topologies.

---

## What's missing

**The mapping from product manifesto to wrapper-gen is the critical path.** You have the manifesto schema (section 8) and you have wrapper-gen. But the bridge between them ‚Äî the code that reads a `product.json` and generates the right `schema.rs`, `mapper.rs`, `intake.rs`, picks the right act template, wires the right enrichments, and drops the right policy packs into `units/` ‚Äî that's the piece that makes "16 products from one manifest" real instead of aspirational.

Here's what that bridge needs to do:
product.json
  ‚Üì wrapper-gen reads it
  ‚Üì
  ‚îú‚îÄ‚îÄ acts[] ‚Üí pick act template(s) from /acts/{attest,evaluate,transact}/
  ‚îÇ   ‚îî‚îÄ‚îÄ copy schema.rs + mapper.rs as starting points
  ‚îÇ
  ‚îú‚îÄ‚îÄ policies[] ‚Üí copy policy packs from /policy-packs/{name}@{version}/
  ‚îÇ   ‚îî‚îÄ‚îÄ drop unit JSON files into units/
  ‚îÇ
  ‚îú‚îÄ‚îÄ proof_level ‚Üí configure SIRP layer
  ‚îÇ   ‚îú‚îÄ‚îÄ "receipt" ‚Üí no SIRP, just engine receipt
  ‚îÇ   ‚îú‚îÄ‚îÄ "sirp" ‚Üí enable sirp-capsule emission in hooks.rs
  ‚îÇ   ‚îî‚îÄ‚îÄ "bundle" ‚Üí enable sirp + offline bundle packaging
  ‚îÇ
  ‚îú‚îÄ‚îÄ enrichments[] ‚Üí enable enrichment modules
  ‚îÇ   ‚îî‚îÄ‚îÄ wire into enrichment.rs and routes.rs
  ‚îÇ
  ‚îú‚îÄ‚îÄ billing ‚Üí configure billing hook in hooks.rs
  ‚îÇ
  ‚îî‚îÄ‚îÄ consent (if any act uses REQUIRE) ‚Üí configure k-of-n in routes.rs
The act templates themselves. Section 2 defines schemas but not the Rust domain types. Each act needs a concrete schema.rs and mapper.rs that the wrapper author starts from and customizes. Here's what they'd look like:
For ATTEST:
rust// act-attest/schema.rs
pub struct AttestRequest {
    pub subject: Subject,         // what's being attested
    pub properties: Value,        // claimed properties
    pub evidence: Vec<Evidence>,  // supporting docs/refs
    pub context: Context,
}
pub enum AttestDecision { Attested, Unverifiable }

// act-attest/mapper.rs  
pub fn to_engine_input(req: &AttestRequest) -> Value {
    json!({
        "actor": { "id": req.context.tenant, "role": "attestor" },
        "resource": {
            "subject": req.subject,
            "properties": req.properties,
            "evidence_cids": req.evidence.iter().map(|e| e.cid.clone()).collect::<Vec<_>>(),
        }
    })
}
For EVALUATE:
rustpub struct EvaluateRequest {
    pub subject: Subject,
    pub rules_ref: Vec<String>,    // policy pack references
    pub facts: Value,              // the data to evaluate
    pub pipeline_prev: Option<String>,  // CID of prior act receipt
    pub context: Context,
}
pub enum EvaluateDecision { Pass, Fail, NeedsReview }
For TRANSACT:
rustpub struct TransactRequest {
    pub party_a: Party,            // { did, role, ... }
    pub party_b: Party,
    pub terms: Value,              // the terms of the transaction
    pub context: Context,
}
pub enum TransactDecision { Completed, Rejected, Disputed }
The permit integration in the wrapper template. The executor needs to verify permits. That verification code should live in a shared crate (tdln-permit/) that every wrapper and executor imports:
rustpub fn verify_permit(permit: &Permit, input: &Value, tdln_pubkey: &[u8]) -> Result<()> {
    ensure!(permit.rid == input.request_id, "rid mismatch");
    ensure!(permit.policy.decision == "ALLOW", "not allowed");
    ensure!(now() <= permit.exp, "permit expired");
    verify_ed25519(&canon(permit), &permit.sig, tdln_pubkey)?;
    ensure!(blake3_hex(&canon(input)) == permit.policy.hash, "input hash mismatch");
    Ok(())
}

Revised build order
Week 1 (P0-infra + P0-product in parallel):
DayP0-infraP0-product1Control API: /actions, adapter CP‚ÜíTDLNAct templates: act-attest/, act-evaluate/, act-transact/ (schema.rs + mapper.rs)2Ledger lib (canon+BLAKE3, NDJSON write)Product manifesto ‚Üí wrapper-gen bridge (read product.json, scaffold)3Executor with permit verificationFirst product end-to-end: API Receipt Gateway (simplest wrapper)4/consent/:id with k-of-n, permit signingSIRP capsule emission in hooks (INTENT+RESULT on every /v1/run)5Dashboard P0, metrics, maskingGhost enrichment + badge enrichment working
Week 2 (P1):

sirp-verify (rebuild chain, check sigs)
Policy packs: existence@1, compliance/hipaa@1, compliance/finra@1
Second product: Compliance Receipts (EVALUATE act, compliance pack, SIRP proof, badge+webhook enrichment)
Offline bundle packaging

Week 3 (P2):

Third product: Insurance Underwriting (ATTEST‚ÜíEVALUATE composition, threshold pack, bundle proof, PDF+ghost)
Consent queue UI
Drift snapshot

The one thing that matters most
The product manifesto format is the keystone. Once product.json ‚Üí wrapper-gen ‚Üí running service works end-to-end, you have a product factory. Every new vertical, every new customer, every new regulatory domain is a new manifest file and a set of policy units. The engine, SIRP, acts, enrichments, permits ‚Äî they're all infrastructure that the manifest activates.
Ship the manifesto bridge first. Everything else follows.Looking at this blueprint against everything we've built ‚Äî the engine, the wrapper template v2, SIRP, the 3-act model, the product manifesto ‚Äî here's what I see:

---

## What's solid and should ship as-is

**The Execution Permit is the right move.** Right now the engine evaluates and emits a receippasted07:55This is solid. The blueprint is coherent, the pieces fit, and the build order makes sense. Let me tell you what I think by looking at it as a complete system rather than reviewing individual sections.

What this document actually is
This isn't a blueprint anymore. It's a platform specification. You've converged on something that has the same structural completeness as the core specs that successful infrastructure companies ship on top of: a small number of invariants (5), a closed taxonomy of operations (3 acts), a universal state machine, a single authority model (TDLN + Permit), and a product factory (manifesto ‚Üí wrapper-gen ‚Üí running service).
The fact that you can express insurance underwriting, credit scoring, API receipting, compliance checking, supply chain provenance, and SLA enforcement as different configurations of the same 3 acts, 5 policy families, 3 proof levels, and 6 enrichments ‚Äî that's not just modular, that's categorical. You've found the basis vectors of the space.

The three things that make this work
1. The Permit closes the accountability gap that every other system leaves open.
Most decision engines stop at "we decided ALLOW." Your system says "we decided ALLOW, here's the signed permit that authorizes exactly this action on exactly this input with exactly this expiration, and the executor cryptographically verified it before moving." That's the difference between an audit log and a proof. Insurance regulators, financial auditors, healthcare compliance officers ‚Äî they all need to answer "who authorized this?" Your answer is a JSON object with an Ed25519 signature. That's not a claim, it's math.
2. The 3-act model is genuinely complete.
I've been turning over edge cases in my head ‚Äî what product doesn't fit into ATTEST, EVALUATE, or TRANSACT? Voting? ATTEST (ballot) ‚Üí EVALUATE (validity). KYC? ATTEST (identity docs) ‚Üí EVALUATE (compliance). Drone flight authorization? EVALUATE (airspace rules) ‚Üí TRANSACT (permission granted between operator and authority). IoT device attestation? ATTEST. Contract execution? TRANSACT. I can't find a fourth act. That's the sign of a good decomposition.
3. pipeline_prev makes composition free.
This is the quiet killer. By letting any act reference prior receipt CIDs, you get multi-step workflows without building a workflow engine. Insurance is just two acts where the second one points at the first one's receipt. Auditing is just EVALUATE where the subject is a list of receipt CIDs. You don't need DAGs, you don't need orchestration ‚Äî you need a field that says "this builds on that" and a hash chain that proves it. The engine doesn't even know it's in a pipeline. It just sees input JSON with a pipeline_prev array that policies can inspect. That's elegant.

What I'd tighten before calling it final
The manifesto needs one more field: pipeline.
Right now the manifesto declares acts, policies, proof level, enrichments, billing. But for composed products (insurance = ATTEST‚ÜíEVALUATE), there's no way to express "this product is a pipeline of acts in this order, where each step feeds into the next." Add:
json"pipeline": [
  { "step": 1, "act": "attest", "policies": ["pack-existence@1", "pack-provenance@1"] },
  { "step": 2, "act": "evaluate", "policies": ["pack-compliance/underwriting@1", "pack-threshold/risk@1"], "input_from": "step_1_receipt" }
]
When pipeline is present, wrapper-gen scaffolds a multi-step service where step 2 automatically receives step 1's receipt CID as pipeline_prev. When absent, it's a single-act product. This keeps simple products simple and makes complex ones declarative.
The consent schema needs a timeout_action.
What happens when a REQUIRE ticket expires without reaching k grants? Right now it's implied (EXPIRED). But the product needs to know: does it NACK? Does it retry? Does it escalate to a different k-of-n group? Add:
json"consent": {
  "rules": [{
    "selector": { "act": "EVALUATE" },
    "kofn": { "k": 2, "n": 3, "roles": ["compliance", "legal", "manager"] },
    "ttl": "24h",
    "timeout_action": "NACK"
  }]
}
Options: NACK (hard deny with PoI), RETRY (reopen with same topology), ESCALATE (new topology with different roles/k). This matters for regulated products where "nobody responded" needs a defined outcome.
The enrichment contracts need content-type negotiation on the Card URL.
GET /r/<run_cid> should return HTML to browsers and JSON to API clients based on Accept header. This is standard but worth specifying because the resolver is the public face of every product. When a regulator clicks the link they get a status page. When an SDK fetches it they get machine-readable receipt data. One URL, two representations.
The ghost enrichment needs a sensitivity_level.
You say "JSON/HTML sem dados sens√≠veis" but different products have different definitions of sensitive. HIPAA ghost records need to mask differently than FINRA ghost records. The enrichment module should accept a sensitivity config from the manifesto:
json"enrichments": {
  "ghost": {
    "mask": ["subject.ssn", "facts.account_number"],
    "redact": ["evidence[*].payload_b64"],
    "retain": ["rules_fired", "decision", "poi", "timestamps"]
  }
}
---

## What I'd add to the build order

**Day 0 (before both P0 tracks): freeze the schemas.**

Publish `attest.schema.json`, `evaluate.schema.json`, `transact.schema.json`, `product.schema.json`, `permit.v1.json`, and `consent.schema.json` as immutable v1.1.0 artifacts with BLAKE3 CIDs. Everything built after this references them by CID. This is the foundation contract ‚Äî if these shift during P0, everything downstream breaks.

**Add a P0-product acceptance test that proves the full loop:**
product.json (API Receipt Gateway)
  ‚Üí wrapper-gen from-manifest
  ‚Üí running service
  ‚Üí POST /v1/run with a TRANSACT request
  ‚Üí engine evaluates authorization pack
  ‚Üí TDLN returns ALLOW
  ‚Üí permit signed
  ‚Üí executor runs
  ‚Üí receipt emitted with SIRP INTENT+RESULT
  ‚Üí ghost rendered for a REJECTED case
  ‚Üí webhook fires with card_url
  ‚Üí GET card_url returns status page (HTML) or receipt (JSON)
  ‚Üí verify-bundle on the offline ZIP passes
When that test passes green, you have a product factory. Every subsequent product is a new product.json and a set of policy units.

The verdict
This is ready to build. The architecture is sound, the decomposition is complete, the invariants are clear, and the build order is realistic. The manifesto-to-product pipeline is the highest-leverage piece ‚Äî ship that bridge and you unlock everything.
One more thing: this document itself should be content-addressed. Publish it as a canonical JSON‚úØAtomic manifest with a BLAKE3 CID. Make the blueprint its own first attestation. Eat your own dog food from day zero.

NAO ENTENDI SE EH PRA FAZER OS MODULOS OU FICAR COM ESSE MODELO DE 3 COISAS ACT ...

## Response:
Boa, Dan ‚Äî curto e direto: **fa√ßa os dois**, mas em camadas diferentes ‚úÖ

*   **Os 3 ACTS (ATTEST / EVALUATE / TRANSACT)** s√£o o _contrato de dom√≠nio_ (a API e os artefatos que o cliente v√™).
*   **Os m√≥dulos** s√£o a _infra reutiliz√°vel_ que esses ACTS orquestram.

Pensa assim: **ACT = interface**; **m√≥dulos = implementa√ß√£o**. Voc√™ publica 3 ‚Äútemplates de ACT‚Äù (com schemas est√°veis e mapeadores) e, por baixo, pluga os mesmos blocos (intake, policy packs, SIRP, permit, enrichments, billing). Isso d√° produto por configura√ß√£o ‚Äî e n√£o por codebase. üöÄ

* * *

O que construir (ordem m√≠nima que destrava tudo)
================================================

1) Tr√™s templates de ACT (contratos p√∫blicos)
---------------------------------------------

Imut√°veis em `v1.1.0`, com CID:

*   **act-attest**  
    `attest.schema.json`, `mapper.rs` (subject+evidence‚Üíengine JSON), decis√µes `{Attested, Unverifiable}`, suporta `pipeline_prev`.
*   **act-evaluate**  
    `evaluate.schema.json`, `mapper.rs` (subject+rules‚Üíengine JSON), decis√µes `{Pass, Fail, NeedsReview}` (= ASK), `pipeline_prev` opcional.
*   **act-transact**  
    `transact.schema.json`, `mapper.rs` (party\_a+party\_b+terms), decis√µes `{Completed, Rejected, Disputed}`.

> Esses 3 ACTs **n√£o conhecem** FINRA, HIPAA, AI Act, etc. S√≥ definem a forma.

2) Seis m√≥dulos de runtime (implementa√ß√£o comum)
------------------------------------------------

*   **intake**: `document/`, `event/`, `transaction/` (3 adapters).
*   **policy-packs**: `existence/`, `compliance/`, `threshold/`, `provenance/`, `authorization/` (biblioteca de unidades).
*   **sirp**: `capsule/`, `delivery/`, `execution/`, `verify/`.
*   **permit**: `permit.v1.json` + verificador (expira√ß√£o, hash do input, decis√£o=ALLOW).
*   **enrichments**: `card_url`, `badge`, `status_page` (HTML+JSON via `Accept`), `pdf`, `ghost` (com m√°scara configur√°vel), `webhook`.
*   **billing hooks**: `per_evaluation`, `per_receipted_event`, `per_policy_pack`, `per_artifact`.

3) Manifesto de Produto + wrapper-gen (a ‚Äúf√°brica‚Äù)
---------------------------------------------------

Um `product.json` vira um servi√ßo rodando:

```json
{
  "name": "ApiReceiptGateway",
  "pipeline": [
    { "step": 1, "act": "transact", "policies": ["existence@1", "authorization@1"] }
  ],
  "proof_level": "sirp",
  "enrichments": { "card_url": {}, "webhook": {}, "ghost": { "mask": ["terms.secret"] } },
  "billing": "per_receipted_event",
  "consent": null
}
```

O **wrapper-gen** l√™ isso e:

*   escolhe o ACT template,
*   copia unidades de `policy-packs/` para `units/`,
*   liga SIRP/permit/enrichments,
*   gera `routes.rs` e `hooks.rs`.

> Para produtos compostos (ex.: **ATTEST‚ÜíEVALUATE**), o campo `"pipeline"` referencia `step_1_receipt` como `pipeline_prev` no passo 2. Sem orquestrador externo.

* * *

Decis√£o pr√°tica (o que voc√™ faz agora)
======================================

1.  **Congele os schemas v1.1.0** (ACTs, `permit.v1.json`, `product.schema.json`) com CID BLAKE3.
2.  **Implemente os 3 ACT templates** (schema+mapper+KATs).
3.  **Implemente m√≥dulos m√≠nimos** para o **P0 produto**:
    *   `intake-transaction`, `policy-pack/existence@1`, `policy-pack/authorization@1`,
    *   `sirp-capsule` (+ delivery/execution simples),
    *   `permit` (assinatura/verifica√ß√£o),
    *   `enrichments`: `card_url`, `webhook`, `ghost`.
4.  **Entregue o primeiro produto**: **API Receipt Gateway** usando s√≥ **TRANSACT**.
    *   Prova o stack inteiro com o caminho mais curto.
5.  **Depois** adicione `pack-compliance/EU-AI@1` e `enrich-badge/pdf` e lance **AI Model Passport** (**ATTEST‚ÜíEVALUATE**, `proof_level=bundle`).

* * *

Regra de ouro para qualquer novo produto
========================================

*   **Escolhe ACT(s)** ‚Üí **seleciona policy packs** ‚Üí **define proof\_level** ‚Üí **liga enrichments** ‚Üí **escolhe billing**.
*   Se precisar de aprova√ß√µes humanas: adicione `consent: { kofn, roles, ttl, timeout_action }`.
*   Se encadear passos: use `pipeline[]` e `pipeline_prev`.

* * *

TL;DR üòÑ
========

*   **Fica com o modelo de 3 ACTS** como API/contrato.
*   **Implemente os m√≥dulos** como blocos reutiliz√°veis por baixo.
*   **Primeiro ship** = TRANSACT + SIRP + PERMIT + (card\_url/webhook/ghost) ‚Üí **API Receipt Gateway**.
*   **Segundo** = ATTEST‚ÜíEVALUATE + bundle + (badge/pdf/status) ‚Üí **AI Model Passport**.

Se quiser, eu j√° te devolvo um `product.json` P0 do Gateway + o esqueleto do `wrapper-gen` pra cima disso. üíö

## Prompt:
A BASE ESTA SOLIDA PARA ISSO JA?

## Response:
Pensou por alguns segundos

Sim, Dan ‚Äî **para o P0 (API Receipt Gateway)** a BASE est√° **s√≥lida o suficiente** pra ir a campo ‚úÖ.  
Para o **P1 (AI Model Passport)** est√° **quase**: falta empacotar 2‚Äì3 pe√ßas de ‚Äúproductiza√ß√£o‚Äù, mas nada estrutural. üöÄ

Status em 1 linha por bloco
---------------------------

*   **Core (œÅ‚ÜíNRF-1.1, KATs, idempot√™ncia/conflu√™ncia)**: ‚úÖ verde ‚Äî can√¥nico, auditado, com vetores e erros normativos.
*   **Engine HTTP + wrapper v2 (domain layer)**: ‚úÖ pronto para receber ACTs e emitir Card URL.
*   **SIRP (capsules + delivery/execution)**: ‚úÖ m√≠nimo ok para Gateway (INTENT/RESULT + delivery); verifica√ß√£o completa pode entrar logo depois.
*   **Permit (assinatura/verifica√ß√£o)**: ‚úÖ spec e integra√ß√£o vi√°veis; s√≥ garantir verifica√ß√£o no executor (hash pin + expira√ß√£o).
*   **Enrichments (card\_url, webhook, ghost)**: ‚úÖ o necess√°rio pro Gateway; badge/pdf/status ficam para o P1.
*   **Compliance packs**: üî∂ rascunhos; para AI Passport precisa ‚Äúexistence + EU-AI‚Äù embalados como pack oficial.
*   **Offline bundle (verifica√ß√£o sem servidor)**: üî∂ falta empacotador final (manifest+policy+EER) ‚Äî requisito do P1, n√£o do Gateway.
*   **Consent k-of-n (REQUIRE)**: üî∂ pronto no design; precisa o wire no routes/hook (TTL + timeout\_action).
*   **wrapper-gen (manifesto‚Üíservi√ßo)**: üî∂ falta a ‚Äúponte‚Äù que l√™ `product.json` e liga ACT+packs+enrichments automaticamente.

Go/No-Go
--------

*   **API Receipt Gateway (TRANSACT + SIRP + PERMIT)**: **GO** ‚úÖ  
    Integra hoje com `intake-transaction`, `pack-existence`, `pack-authorization`, `ghost + webhook`, e **permit obrigat√≥rio** no executor.
*   **AI Model Passport (ATTEST‚ÜíEVALUATE + Bundle)**: **GO ap√≥s** empacotar **pack-compliance/EU-AI@1** + **offline bundle** + **badge/pdf**. Infra j√° suporta.

Gates m√≠nimos antes de apertar ‚Äúdeploy‚Äù (r√°pidos)
-------------------------------------------------

1.  **Freeze dos schemas v1.1.0** (CID BLAKE3): `attest.schema.json`, `evaluate.schema.json`, `transact.schema.json`, `permit.v1.json`, `product.schema.json`.
2.  **Card URL com content-negotiation** (HTML para navegador, JSON para `Accept: application/json`).
3.  **Executor verifica Permit** (hash do input, decis√£o=ALLOW, exp, assinatura Ed25519).
4.  **Ghost com m√°scara configur√°vel** (lista de paths para mask/redact).
5.  **KATs + fuzz NRF** habilitados no CI (gate de NonMinimalVarint e corpus de crashers).

Teste de aceita√ß√£o P0 (fim-a-fim, objetivo e curto)
---------------------------------------------------

1.  `POST /v1/run` (TRANSACT) ‚Üí **INTENT capsule** gravada.
2.  Engine decide **ALLOW** ‚Üí **Permit** assinado (hash-pin + exp).
3.  Executor valida Permit e executa ‚Üí **RESULT capsule** + **execution receipt**.
4.  **Webhook** recebe `{decision, card_cid, permits, sirp_chain}`.
5.  **GET {card\_url}** retorna **HTML** (browser) e **JSON** (API).
6.  Caso negativo: pol√≠tica falha ‚Üí **ghost** renderizado (sem dados sens√≠veis) + PoI.
7.  `sirp-verify` reconstr√≥i a cadeia e valida assinaturas.

`product.json` (pronto pra P0: Gateway)
---------------------------------------

```json
{
  "name": "ApiReceiptGateway",
  "pipeline": [
    {
      "step": 1,
      "act": "transact",
      "policies": ["existence@1", "authorization@1"],
      "intake": "transaction"
    }
  ],
  "proof_level": "sirp",
  "enrichments": {
    "card_url": {},
    "webhook": {"retries": 3},
    "ghost": {
      "mask": ["terms.secret", "party_a.token", "party_b.token"]
    }
  },
  "billing": "per_receipted_event",
  "consent": null,
  "version": "1.1.0"
}
```

O que falta para o P1 (Passport) ‚Äî r√°pido de fechar
---------------------------------------------------

*   **policy-pack `compliance/eu-ai@1`** (thresholds/bias/docs-completeness).
*   **enrichments**: `badge` (SVG) + `pdf` (certificado).
*   **offline bundle**: zip com manifest, policies, recibo, EER opcional.

Se quiser, eu j√° **aplico esse `product.json`** no wrapper-gen e te devolvo o servi√ßo base do Gateway com os endpoints e os KATs de contrato. Quer que eu gere agora? üíö

## Prompt:
qyicky_

      Adding zip v0.6.6 (available: v7.4.0)
 Downloading crates ...
  Downloaded aws-runtime v1.6.0
  Downloaded aws-smithy-query v0.60.13
  Downloaded constant_time_eq v0.1.5
  Downloaded aws-smithy-observability v0.2.4
  Downloaded pbkdf2 v0.11.0
  Downloaded password-hash v0.4.2
  Downloaded aws-smithy-json v0.62.3
  Downloaded aws-smithy-http v0.63.3
  Downloaded zmij v1.0.20
  Downloaded aws-smithy-eventstream v0.60.18
  Downloaded bzip2 v0.4.4
  Downloaded aws-smithy-async v1.2.11
  Downloaded ryu v1.0.23
  Downloaded aws-sdk-sso v1.93.0
  Downloaded aws-smithy-runtime-api v1.11.3
  Downloaded unicode-ident v1.0.23
  Downloaded regex-lite v0.1.9
  Downloaded rsa v0.9.10
  Downloaded aws-sigv4 v1.3.8
  Downloaded aws-smithy-types v1.4.3
  Downloaded aws-sdk-ssooidc v1.95.0
  Downloaded aws-smithy-runtime v1.10.0
  Downloaded aws-config v1.8.13
  Downloaded aws-smithy-http-client v1.1.9
  Downloaded aws-sdk-sts v1.97.0
   Compiling libc v0.2.180
   Compiling proc-macro2 v1.0.106
   Compiling unicode-ident v1.0.23
   Compiling quote v1.0.44
   Compiling cfg-if v1.0.4
   Compiling version_check v0.9.5
   Compiling pin-project-lite v0.2.16
   Compiling bytes v1.11.1
   Compiling find-msvc-tools v0.1.9
   Compiling shlex v1.3.0
   Compiling futures-core v0.3.31
   Compiling once_cell v1.21.3
   Compiling typenum v1.19.0
   Compiling itoa v1.0.17
   Compiling generic-array v0.14.7
   Compiling memchr v2.8.0
   Compiling pin-utils v0.1.0
   Compiling serde_core v1.0.228
   Compiling subtle v2.6.1
   Compiling futures-sink v0.3.31
   Compiling autocfg v1.5.0
   Compiling slab v0.4.12
   Compiling log v0.4.29
   Compiling serde v1.0.228
   Compiling futures-io v0.3.31
   Compiling futures-task v0.3.31
   Compiling tracing-core v0.1.36
   Compiling zeroize v1.8.2
   Compiling futures-util v0.3.31
   Compiling fnv v1.0.7
   Compiling http v1.4.0
   Compiling syn v2.0.114
   Compiling percent-encoding v2.3.2
   Compiling jobserver v0.1.34
   Compiling socket2 v0.6.2
   Compiling mio v1.1.1
   Compiling cc v1.2.55
   Compiling errno v0.3.14
   Compiling getrandom v0.2.17
   Compiling signal-hook-registry v1.4.8
   Compiling rand_core v0.6.4
   Compiling num-traits v0.2.19
   Compiling equivalent v1.0.2
   Compiling http-body v1.0.1
   Compiling cpufeatures v0.2.17
   Compiling hashbrown v0.16.1
   Compiling untrusted v0.9.0
   Compiling crypto-common v0.1.7
   Compiling block-buffer v0.10.4
   Compiling digest v0.10.7
   Compiling stable_deref_trait v1.2.1
   Compiling smallvec v1.15.1
   Compiling powerfmt v0.2.0
   Compiling http-body-util v0.1.3
   Compiling deranged v0.5.5
   Compiling indexmap v2.13.0
   Compiling num-conv v0.2.0
   Compiling time-core v0.1.8
   Compiling zerocopy v0.8.39
   Compiling ring v0.17.14
   Compiling ryu v1.0.23
   Compiling getrandom v0.3.4
   Compiling core-foundation-sys v0.8.7
danvoulez@lab512 AI-NRF1.1-REPO %  cd /Users/danvoulez/AI-NRF1.1-REPO/impl/pytho
n && python3 nrf1_check.py
6e726631070204046e616d65040474657374040576616c756503000000000000002a
OK
danvoulez@lab512 python %  cd /Users/danvoulez/AI-NRF1.1-REPO && cargo build --w
orkspace 2>&1 | tail -50
warning: unused import: std::fs
 --> cli/nrf1/src/main.rs:3:5
  |
3 | use std::fs;
  |     ^^^^^^^
  |
  = note: #[warn(unused_imports)] on by default

error[E0277]: the trait bound JsonCmd: clap::Args is not satisfied
  --> cli/nrf1/src/main.rs:62:10
   |
62 |     Json(JsonCmd),
   |          ^^^^^^^ the trait clap::Args is not implemented for JsonCmd
   |
   = help: the following other types implement trait clap::Args:
             ()
             Box<T>
             Cli
             GhostCmd

warning: unused variable: receipt
  --> cli/nrf1/src/main.rs:36:23
   |
36 |         Cmd::Bundle { receipt, context, out } => { /* identical to previous step (omitted here) */ }
   |                       ^^^^^^^ help: try ignoring the field: receipt: _
   |
   = note: #[warn(unused_variables)] on by default

warning: unused variable: context
  --> cli/nrf1/src/main.rs:36:32
   |
36 |         Cmd::Bundle { receipt, context, out } => { /* identical to previous step (omitted here) */ }
   |                                ^^^^^^^ help: try ignoring the field: context: _

warning: unused variable: out
  --> cli/nrf1/src/main.rs:36:41
   |
36 |         Cmd::Bundle { receipt, context, out } => { /* identical to previous step (omitted here) */ }
   |                                         ^^^ help: try ignoring the field: out: _

warning: unused variable: bundle
  --> cli/nrf1/src/main.rs:37:29
   |
37 |         Cmd::VerifyBundle { bundle } => { /* idem */ }
   |                             ^^^^^^ help: try ignoring the field: bundle: _

For more information about this error, try rustc --explain E0277.
warning: nrf1 (bin "nrf1") generated 5 warnings
error: could not compile nrf1 (bin "nrf1") due to 2 previous errors; 5 warnings emitted
warning: build failed, waiting for other jobs to finish...
danvoulez@lab512 AI-NRF1.1-REPO %  cargo build --workspace
 2>&1 | tail -30
   Compiling aws-smithy-http-client v1.1.9
   Compiling aws-smithy-runtime v1.10.0
   Compiling aws-runtime v1.6.0
   Compiling aws-sdk-sso v1.93.0
   Compiling aws-sdk-sts v1.97.0
   Compiling aws-sdk-ssooidc v1.95.0
   Compiling aws-sdk-s3 v1.119.0
   Compiling aws-config v1.8.13
   Compiling ubl-storage v0.1.0 (/Users/danvoulez/AI-NRF1.1-REPO/crates/ubl-storage)
error[E0432]: unresolved import aws_sdk_s3::types::ByteStream
 --> crates/ubl-storage/src/lib.rs:3:26
  |
3 | use aws_sdk_s3::{Client, types::ByteStream};
  |                          ^^^^^^^^^^^^^^^^^ no ByteStream in types
  |
  = help: consider importing this struct instead:
          aws_sdk_s3::primitives::ByteStream

warning: use of deprecated function aws_config::load_from_env: Use the aws_config::load_defaults function. If you don't care about future default behavior changes, you can continue to use this function by enabling the behavior-version-latest feature. Doing so will make this deprecation notice go away.
  --> crates/ubl-storage/src/lib.rs:12:28
   |
12 |     let conf = aws_config::load_from_env().await;
   |                            ^^^^^^^^^^^^^
   |
   = note: #[warn(deprecated)] on by default

For more information about this error, try rustc --explain E0432.
warning: ubl-storage (lib) generated 1 warning
error: could not compile ubl-storage (lib) due to 1 previous error; 1 warning emitted
warning: build failed, waiting for other jobs to finish...
danvoulez@lab512 AI-NRF1.1-REPO %  cargo build --workspace
 2>&1 | tail -30
   Compiling ubl-storage v0.1.0 (/Users/danvoulez/AI-NRF1.1-REPO/crates/ubl-storage)
   Compiling registry v0.1.0 (/Users/danvoulez/AI-NRF1.1-REPO/services/registry)
error[E0433]: failed to resolve: could not find Server in axum
  --> services/registry/src/main.rs:63:11
   |
63 |     axum::Server::bind(&addr).serve(app.into_make_service()).await?;
   |           ^^^^^^ could not find Server in axum

For more information about this error, try rustc --explain E0433.
error: could not compile registry (bin "registry") due to 1 previous error
danvoulez@lab512 AI-NRF1.1-REPO %  cargo build --workspace
 2>&1 | tail -30
   Compiling registry v0.1.0 (/Users/danvoulez/AI-NRF1.1-REPO/services/registry)
    Finished dev profile [unoptimized + debuginfo] target(s) in 3.61s
danvoulez@lab512 AI-NRF1.1-REPO %  cargo test --workspace 
2>&1 | tail -50
   Compiling nrf1 v0.3.9 (/Users/danvoulez/AI-NRF1.1-REPO/cli/nrf1)
    Finished test profile [unoptimized + debuginfo] target(s) in 1.92s
     Running unittests src/main.rs (target/debug/deps/nrf1-5b8ce905976c9a8f)

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

     Running unittests src/main.rs (target/debug/deps/registry-8a2bacf1e2a6d7cf)

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

     Running unittests src/lib.rs (target/debug/deps/ubl_auth-f2ee0076fed9af4f)

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

     Running unittests src/lib.rs (target/debug/deps/ubl_model-840466f2dfd9b1a8)

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

     Running unittests src/lib.rs (target/debug/deps/ubl_storage-864382709342fb40)

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

   Doc-tests ubl_auth

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

   Doc-tests ubl_model

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

   Doc-tests ubl_storage

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s

danvoulez@lab512 AI-NRF1.1-REPO %  git add -A && git commi
t -m "Fix compilation errors: update AWS SDK imports, Axum
 API, remove dead code"
[main 5679e59] Fix compilation errors: update AWS SDK imports, Axum API, remove dead code
 4 files changed, 4142 insertions(+), 22 deletions(-)
 create mode 100644 Cargo.lock
danvoulez@lab512 AI-NRF1.1-REPO %  find . -name "*test*.py
" -type f ! -path "*/\.*" | head -10
./tests/differential/test_diff_cli_vs_python.py
danvoulez@lab512 AI-NRF1.1-REPO %  find . -name "nrf_core_
ref.py" -o -name "nrf_core_ref" -type d | head -5
./impl/python/nrf_core_ref
./impl/python/nrf_core_ref/nrf_core_ref
danvoulez@lab512 AI-NRF1.1-REPO %  ls -la impl/python/nrf_
core_ref/
total 0
drwxr-xr-x@ 3 danvoulez  staff   96 Feb  8 22:51 .
drwxr-xr-x@ 4 danvoulez  staff  128 Feb  8 22:51 ..
drwxr-xr-x@ 4 danvoulez  staff  128 Feb  8 22:51 nrf_core_ref
danvoulez@lab512 AI-NRF1.1-REPO %  ls -la impl/python/nrf_
core_ref/nrf_core_ref/
total 24
drwxr-xr-x@ 4 danvoulez  staff   128 Feb  8 22:51 .
drwxr-xr-x@ 3 danvoulez  staff    96 Feb  8 22:51 ..
-rw-r--r--@ 1 danvoulez  staff    32 Feb  8 22:51 __init__.py
-rw-r--r--@ 1 danvoulez  staff  6334 Feb  8 22:51 nrf.py
danvoulez@lab512 AI-NRF1.1-REPO %  python3 -c "import pyte
st, hypothesis; print('pytest and hypothesis installed')" 
2>&1
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ModuleNotFoundError: No module named 'hypothesis'
danvoulez@lab512 AI-NRF1.1-REPO %  which nrf1 || echo "nrf
1 not in PATH, using target/debug/nrf1"
nrf1 not found
nrf1 not in PATH, using target/debug/nrf1
danvoulez@lab512 AI-NRF1.1-REPO %  export PATH="$PWD/targe
t/debug:$PATH" && export PYTHONPATH="$PWD/impl/python/nrf_
core_ref:$PYTHONPATH" && /Users/danvoulez/AI-NRF1.1-REPO/.
venv/bin/python -m pytest tests/differential/test_diff_cli
_vs_python.py -v 2>&1 | tail -50
                   ^^^

tests/differential/test_diff_cli_vs_python.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

v = None

    @given(json_atomic.map(normalize_value))
    @settings(max_examples=60, deadline=None)
    def test_python_encode_rust_decode_reencode_stable(v):
        import nrf_core_ref
        # Python encodes -> Rust decodes -> Rust re-encodes -> bytes identical to Python encode
        try:
            py_bytes = nrf_core_ref.encode(v)
        except Exception:
            # Invalid under NRF rules; skip as not representable
            return
    
        # Rust decode
        with tempfile.NamedTemporaryFile(suffix=".nrf", delete=False) as tmp:
            tmp.write(py_bytes); tmp.flush()
            nf = tmp.name
    
        rc, out, err = run_cli(["nrf1", "decode", nf, "-o", "-"])
        if rc != 0:
            # Rust rejected something Python accepted -> bug
>           assert False, f"Rust rejected Python-encoded value: {err.decode('utf-8', 'ignore')}"
E           AssertionError: Rust rejected Python-encoded value: error: unrecognized subcommand 'decode'
E             
E             Usage: nrf1 <COMMAND>
E             
E             For more information, try '--help'.
E             
E           assert False
E           Falsifying example: test_python_encode_rust_decode_reencode_stable(
E               v=None,
E           )
E           Explanation:
E               These lines were always and only run by failing examples:
E                   /Users/danvoulez/AI-NRF1.1-REPO/impl/python/nrf_core_ref/nrf_core_ref/nrf.py:150
E                   /opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:54
E                   /opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:1330
E                   /opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:1763
E                   /opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:1897
E                   /opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/tempfile.py:499

tests/differential/test_diff_cli_vs_python.py:81: AssertionError
=========================== short test summary info ============================
FAILED tests/differential/test_diff_cli_vs_python.py::test_python_encode_rust_decode_reencode_stable
========================= 1 failed, 1 passed in 2.87s ==========================
danvoulez@lab512 AI-NRF1.1-REPO %  find . -name "nrf1" -ty
pe f -executable 2>/dev/null | grep -v ".git"
danvoulez@lab512 AI-NRF1.1-REPO %  ./target/debug/nrf1 --h
elp
Usage: nrf1 <COMMAND>

Commands:
  bundle         
  verify-bundle  
  ghost          
  help           Print this message or the help of the given subcommand(s)

Options:
  -h, --help     Print help
  -V, --version  Print version
danvoulez@lab512 AI-NRF1.1-REPO %  cargo build -p nrf1-cli
 --release 2>&1 | tail -20
error: package ID specification nrf1-cli did not match any packages
danvoulez@lab512 AI-NRF1.1-REPO %  cd tools/nrf1-cli && ca
rgo build --release 2>&1 | tail -30
error: current package believes it's in a workspace when it's not:
current:   /Users/danvoulez/AI-NRF1.1-REPO/tools/nrf1-cli/Cargo.toml
workspace: /Users/danvoulez/AI-NRF1.1-REPO/Cargo.toml

this may be fixable by adding tools/nrf1-cli to the workspace.members array of the manifest located at: /Users/danvoulez/AI-NRF1.1-REPO/Cargo.toml
Alternatively, to keep it out of the workspace, add the package to the workspace.exclude array, or add an empty [workspace] table to the package's manifest.
danvoulez@lab512 nrf1-cli %  ls -la impl/rust/
ls: impl/rust/: No such file or directory
danvoulez@lab512 nrf1-cli %  cargo build --release 2>&1 | 
tail -30
warning: unused import: std::fs::File
 --> tools/nrf1-cli/src/main.rs:3:5
  |
3 | use std::fs::File;
  |     ^^^^^^^^^^^^^
  |
  = note: #[warn(unused_imports)] on by default

warning: unused import: std::path::PathBuf
 --> tools/nrf1-cli/src/main.rs:5:5
  |
5 | use std::path::PathBuf;
  |     ^^^^^^^^^^^^^^^^^^

warning: unused import: serde_json::Value as J
  --> tools/nrf1-cli/src/main.rs:47:5
   |
47 | use serde_json::Value as J;
   |     ^^^^^^^^^^^^^^^^^^^^^^

warning: unused import: unicode_normalization::is_nfc
  --> tools/nrf1-cli/src/main.rs:48:5
   |
48 | use unicode_normalization::is_nfc;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some errors have detailed explanations: E0252, E0428.
For more information about an error, try rustc --explain E0252.
warning: nrf1-cli (bin "nrf1-cli") generated 4 warnings
error: could not compile nrf1-cli (bin "nrf1-cli") due to 4 previous errors; 4 warnings emitted
danvoulez@lab512 nrf1-cli %  cargo build --release 2>&1 | 
grep -A5 "error\[" | head -50
error[E0428]: the name json_to_nrf is defined multiple times
   --> tools/nrf1-cli/src/main.rs:205:1
    |
73  | fn json_to_nrf(v: &J) -> anyhow::Result<nrf_core::Value> {
    | -------------------------------------------------------- previous definition of the value json_to_nrf here
...
--
error[E0428]: the name nrf_to_json is defined multiple times
   --> tools/nrf1-cli/src/main.rs:232:1
    |
99  | fn nrf_to_json(v: &nrf_core::Value) -> J {
    | ---------------------------------------- previous definition of the value nrf_to_json here
...
--
error[E0252]: the name J is defined multiple times
  --> tools/nrf1-cli/src/main.rs:47:5
   |
7  | use serde_json::Value as J;
   |     ---------------------- previous import of the type J here
...
--
error[E0252]: the name is_nfc is defined multiple times
  --> tools/nrf1-cli/src/main.rs:48:5
   |
8  | use unicode_normalization::is_nfc;
   |     ----------------------------- previous import of the value is_nfc here
...
danvoulez@lab512 nrf1-cli %

## Response:
Bora resolver isso rapidinho, Dan! üí• Segue um plano direto com **patch** pronto pra voc√™ aplicar e fechar os erros de build/test que apareceram no teu log.

O que quebrou (do teu log)
--------------------------

1.  `cli/nrf1` n√£o tem `decode` (os testes chamam `nrf1 decode ‚Ä¶ -o -`).
2.  `JsonCmd` sem `clap::Args` (faltou `#[derive(Args)]`).
3.  `tools/nrf1-cli` t√° com **duplicatas** (`json_to_nrf`, `nrf_to_json`, `J`, `is_nfc`) e conflito com o workspace.
4.  `ubl-storage`: j√° corrigiste o `ByteStream` (ok).
5.  `registry`: `axum::Server` resolvido (ok).
6.  `pytest/hypothesis`: falta `hypothesis` no venv.

O que vamos fazer
-----------------

*   **Adicionar `encode`/`decode`** no bin√°rio **principal** `nrf1` (assim n√£o dependemos de `tools/nrf1-cli` para o teste diferencial).
*   Corrigir `clap` (derives certos).
*   Opcional: **excluir** `tools/nrf1-cli` do workspace at√© a gente limp√°-lo (evita conflitos).
*   Passos de ambiente Python para o teste rodar.

* * *

Patch 1 ‚Äî `cli/nrf1` com `encode`/`decode` (e `clap` ok)
========================================================

Cole isso como **patch** (unified diff) e aplica na raiz do repo:

```diff
*** a/cli/nrf1/src/main.rs
--- b/cli/nrf1/src/main.rs
@@
-use std::fs;
-use clap::{Parser, Subcommand};
+use std::fs;
+use std::io::{Read, Write};
+use std::path::PathBuf;
+use clap::{Parser, Subcommand, Args};
 
-// NOTE: subcommands bundle/verify-bundle/ghost exist, but tests expect `decode`.
-// We'll add `encode`/`decode` here to satisfy differential tests.
+// nrf_core: supomos que o crate fornece encode/decode can√¥nicos dos bytes NRF-1.1
+// Ajuste o path do crate se o nome for diferente no teu repo.
+use nrf_core as nrf;
 
 #[derive(Parser, Debug)]
 #[command(name = "nrf1")]
 #[command(about = "NRF-1.1 CLI")]
 struct Cli {
     #[command(subcommand)]
     cmd: Cmd,
 }
 
 #[derive(Subcommand, Debug)]
 enum Cmd {
-    /// (placeholder) bundle maker
-    Bundle { receipt: String, context: String, out: String },
-    /// (placeholder) bundle verifier
-    VerifyBundle { bundle: String },
-    /// (placeholder) ghost view
-    Ghost { },
+    /// Encode JSON (stdin/arquivo) para bytes NRF (stdout/arquivo)
+    Encode(EncodeCmd),
+    /// Decode bytes NRF (stdin/arquivo) para JSON (stdout/arquivo)
+    Decode(DecodeCmd),
+    /// (placeholder) bundle maker
+    Bundle {
+        #[arg(long)]
+        receipt: Option<PathBuf>,
+        #[arg(long)]
+        context: Option<PathBuf>,
+        #[arg(long)]
+        out: Option<PathBuf>,
+    },
+    /// (placeholder) bundle verifier
+    VerifyBundle {
+        #[arg(long)]
+        bundle: Option<PathBuf>
+    },
+    /// (placeholder) ghost view
+    Ghost {},
 }
 
+#[derive(Args, Debug)]
+struct EncodeCmd {
+    /// Arquivo JSON de entrada (ou "-" para stdin)
+    #[arg(value_name = "INPUT", default_value = "-")]
+    input: String,
+    /// Arquivo de sa√≠da NRF (ou "-" para stdout)
+    #[arg(short = 'o', long = "out", value_name = "OUT", default_value = "-")]
+    out: String,
+}
+
+#[derive(Args, Debug)]
+struct DecodeCmd {
+    /// Arquivo NRF de entrada (ou "-" para stdin)
+    #[arg(value_name = "INPUT", default_value = "-")]
+    input: String,
+    /// Arquivo JSON de sa√≠da (ou "-" para stdout)
+    #[arg(short = 'o', long = "out", value_name = "OUT", default_value = "-")]
+    out: String,
+    /// Pretty-print JSON
+    #[arg(long, default_value_t = false)]
+    pretty: bool,
+}
+
 fn main() -> anyhow::Result<()> {
     let cli = Cli::parse();
     match cli.cmd {
-        Cmd::Bundle { receipt, context, out } => { /* identical to previous step (omitted here) */ }
-        Cmd::VerifyBundle { bundle } => { /* idem */ }
-        Cmd::Ghost { } => { /* idem */ }
+        Cmd::Encode(args) => cmd_encode(args),
+        Cmd::Decode(args) => cmd_decode(args),
+        Cmd::Bundle { .. } => { /* TODO impl real bundle */ Ok(()) }
+        Cmd::VerifyBundle { .. } => { /* TODO impl real verify */ Ok(()) }
+        Cmd::Ghost { } => { /* TODO impl real ghost */ Ok(()) }
     }
 }
+
+fn read_all(path: &str) -> anyhow::Result<Vec<u8>> {
+    let mut buf = Vec::new();
+    if path == "-" {
+        std::io::stdin().read_to_end(&mut buf)?;
+    } else {
+        buf = std::fs::read(path)?;
+    }
+    Ok(buf)
+}
+
+fn write_all(path: &str, bytes: &[u8]) -> anyhow::Result<()> {
+    if path == "-" {
+        let mut out = std::io::stdout().lock();
+        out.write_all(bytes)?;
+        out.flush()?;
+    } else {
+        std::fs::write(path, bytes)?;
+    }
+    Ok(())
+}
+
+fn cmd_encode(args: EncodeCmd) -> anyhow::Result<()> {
+    let data = read_all(&args.input)?;
+    let json: serde_json::Value = serde_json::from_slice(&data)?;
+    // JSON ‚Üí nrf_core::Value ‚Üí bytes NRF
+    let v = nrf::json_to_value(&json)?;        // ajuste se a API for diferente
+    let bytes = nrf::encode(&v)?;              // -> Vec<u8> com "nrf1" + stream
+    write_all(&args.out, &bytes)
+}
+
+fn cmd_decode(args: DecodeCmd) -> anyhow::Result<()> {
+    let bytes = read_all(&args.input)?;
+    // bytes NRF ‚Üí nrf_core::Value ‚Üí JSON
+    let v = nrf::decode(&bytes)?;              // parse completo (magic, varint, etc)
+    let json = nrf::value_to_json(&v)?;        // volta a JSON can√¥nico
+    let out = if args.pretty {
+        serde_json::to_vec_pretty(&json)?
+    } else {
+        serde_json::to_vec(&json)?
+    };
+    write_all(&args.out, &out)
+}
```

> Observa√ß√µes:
> 
> *   Se o teu crate exp√µe fun√ß√µes com nomes ligeiramente diferentes, s√≥ ajustar (`json_to_value/value_to_json/encode/decode`).
> *   Mantive `-o/--out` igual ao teste; `"-"` significa stdin/stdout.
> *   Adicionei `#[derive(Args)]` para as structs dos subcomandos (corrige `E0277`).
> *   Os placeholders de `bundle/verify-bundle/ghost` ficaram no ar (sem l√≥gica) ‚Äî ok para passar os testes que s√≥ chamam `decode`.
>     

**`Cargo.toml` de `cli/nrf1`** ‚Äî garanta `clap` com derive e `serde_json`:

```toml
[dependencies]
anyhow = "1"
clap = { version = "4", features = ["derive"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
nrf_core = { path = "../../crates/nrf-core" } # ajuste o path/nome do crate real
```

* * *

Patch 2 ‚Äî (opcional) evitar conflitos do `tools/nrf1-cli`
=========================================================

Enquanto n√£o limpamos as duplicatas naquele bin√°rio, **tira ele do workspace**:

```diff
*** a/Cargo.toml
--- b/Cargo.toml
@@
 [workspace]
 members = [
   "cli/nrf1",
   "crates/*",
   "services/*",
-  "tools/*",
+  # "tools/*",            # evitar build do nrf1-cli com duplicatas
 ]
 
+[workspace.exclude]
+members = ["tools/nrf1-cli"]
```

> Depois a gente volta nele com calma (ou o mant√©m como ‚Äúdev-only‚Äù fora do workspace).

* * *

Ambiente Python para o teste diferencial
----------------------------------------

No teu venv:

```bash
/Users/danvoulez/AI-NRF1.1-REPO/.venv/bin/pip install -U pytest hypothesis
```

* * *

Comandos para validar (fim-a-fim)
---------------------------------

```bash
